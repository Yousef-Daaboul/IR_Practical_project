{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"n7jPDIw2RNgj"},"outputs":[],"source":["vectorizer_path = '/content/drive/MyDrive/Dream Graduation Project/IR/clinical_vectorizer.pickle'\n","\n","db_path = '/content/drive/MyDrive/Dream Graduation Project/IR/clinicaltrials.db'\n","\n","qrels_path = '/content/drive/MyDrive/Dream Graduation Project/IR/evaluation/clinical_trials_qrels.pickle'\n","\n","queries_path = '/content/drive/MyDrive/Dream Graduation Project/IR/evaluation/queries.csv'"]},{"cell_type":"code","source":["import pickle\n","\n","with open(qrels_path, 'rb') as f:\n","  qrels = pickle.load(f)\n","\n","\n","with open(vectorizer_path, 'rb') as f:\n","  vectorizer = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/Dream Graduation Project/IR/clinical_trials_main_matrix.pickle', 'rb') as f:\n","  matrix = pickle.load(f)\n"],"metadata":{"id":"h2R6TKKUVlJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sqlite3\n","\n","sqlite_connection = sqlite3.connect(\"/content/drive/MyDrive/Dream Graduation Project/IR/clinicaltrials.db\")\n","\n","cursor = sqlite_connection.cursor()\n","\n","def sort_dicts_by_list(data, order):\n","    \"\"\"Sorts a list of dictionaries based on the order of a corresponding list of numbers.\n","\n","    Args:\n","        data: A list of dictionaries, where each dictionary has an 'id' key.\n","        order: A list of numbers that defines the desired order for the dictionaries.\n","\n","    Returns:\n","        A new list containing the sorted dictionaries.\n","\n","    Raises:\n","        TypeError: If the lengths of 'data' and 'order' are not equal.\n","        ValueError: If any element in 'order' is not found in the 'id' values of 'data'.\n","    \"\"\"\n","\n","    if len(data) != len(order):\n","        raise TypeError(\"Lengths of data and order lists must be equal.\")\n","\n","    id_to_dict = {d['id']: d for d in data}  # Create a dictionary for efficient lookup\n","\n","    if not all(num in id_to_dict for num in order):\n","        raise ValueError(f\"Values in 'order' not found in any dictionary 'id'.\")\n","\n","    sorted_data = [id_to_dict[num] for num in order]\n","\n","    return sorted_data\n","\n","\n","def convert_to_json(data: list[tuple]) -> list[dict]:\n","    desired_structure = {'id': None, 'doc_id': None, 'title': None, 'description': None, 'summary': None}\n","    converted_data = [\n","        dict(zip(desired_structure.keys(), item))\n","        for item in data\n","    ]\n","    return converted_data\n","\n","\n","def get_from_db(indices):\n","    table_name = 'clinical_trials'\n","    cursor = sqlite_connection.cursor()\n","    query_result = cursor.execute(f'SELECT * FROM {table_name} where id in {tuple(indices)}').fetchall()\n","    result = convert_to_json(query_result)\n","    data = sort_dicts_by_list(result, indices)\n","    cursor.close()\n","    return data"],"metadata":{"id":"We6A1stoYehJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Clean Data"],"metadata":{"id":"1KjXsaHaq_4T"}},{"cell_type":"markdown","source":["## install **spellchecker** library\n","\n"],"metadata":{"id":"zocKInS_BmaT"}},{"cell_type":"code","source":["# !pip install pyspellchecker"],"metadata":{"id":"yS7y_J8rpQiP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## import Libraries for clean data\n"],"metadata":{"id":"a4DMIE7CBVq3"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMJ4y-p6rEoD","executionInfo":{"status":"ok","timestamp":1717404975101,"user_tz":-180,"elapsed":2099,"user":{"displayName":"Nezar Abo-Haileh","userId":"08140513927369965229"}},"outputId":"ef8fbbd1-229b-48f8-825c-ecc99ac3815d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["\n","## First remove_punctuation_tokenizer"],"metadata":{"id":"LmxfeAaJkYiM"}},{"cell_type":"code","source":["import string\n","\n","def remove_punctuation_tokenizer(txt: str):\n","    new_tokens = []\n","    txt = txt.lower()\n","    for token in txt.split():\n","        new_tokens.append(token.translate(str.maketrans('', '', string.punctuation)))\n","\n","    return new_tokens"],"metadata":{"id":"Xy5bNhM8n3SF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Second remove_stopwords"],"metadata":{"id":"WmCl0qASoKG2"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from typing import List\n","\n","def remove_stopwords(tokens: List[str]) -> List[str]:\n","    filtered = []\n","    for word in tokens:\n","        if word not in stopwords.words('english'):\n","            filtered.append(word)\n","\n","    return filtered"],"metadata":{"id":"jfYm0XZAoQB1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Third lemmatization"],"metadata":{"id":"5qLLBcgMoccu"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","\n","def get_wordnet_pos(tag_parameter):\n","    tag = tag_parameter[0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","def lemmatization(tokens: List[str]) -> List[str]:\n","    pos_tags = pos_tag(tokens)\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n","\n","    return lemmatized_words"],"metadata":{"id":"-55sVeVnojuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fourth correct_sentence_spelling"],"metadata":{"id":"QGPMPIrBo4fO"}},{"cell_type":"code","source":["# from spellchecker import SpellChecker\n","\n","# def correct_sentence_spelling(tokens: List[str]) -> List[str]:\n","#     spell = SpellChecker()\n","#     c = 0\n","#     misspelled = spell.unknown(tokens)\n","#     for i, token in enumerate(tokens):\n","#         if token in misspelled:\n","#             corrected = spell.correction(token)\n","#             if corrected is not None:\n","#                 c += 1\n","#                 tokens[i] = corrected\n","\n","#     return tokens"],"metadata":{"id":"Af4ESx-fo7qu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finish create text_proccess function"],"metadata":{"id":"SkDrYEcOpp0L"}},{"cell_type":"code","source":["def text_processor(txt: str, enable_spell_checking=False):\n","    tokens = remove_punctuation_tokenizer(txt)\n","    tokens = remove_stopwords(tokens)\n","    # if enable_spell_checking:\n","    #     tokens = correct_sentence_spelling(tokens)\n","    tokens = lemmatization(tokens)\n","    return \" \".join(tokens)"],"metadata":{"id":"bAlZFgYzpvzA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"RDMX6uqUajX4"}},{"cell_type":"code","source":["import numpy as np\n","\n","\n","def precision_at_k(relevant_docs, retrieved_docs, k=10) -> float | int:\n","    \"\"\"\n","    Calculates Precision@k\n","\n","    Args:\n","        relevant_docs: A dictionary mapping query IDs to a list of relevant document IDs.\n","        retrieved_docs: A dictionary mapping query IDs to a list of retrieved document IDs, ranked by relevance.\n","        k: The number of top retrieved documents to consider (default 10).\n","\n","    Returns:\n","        The Precision value.\n","    \"\"\"\n","\n","    retrieved = retrieved_docs[:k]\n","    num_retrieved = len(retrieved)\n","    num_retrieved_relevant = calculate_relevant_count(retrieved, relevant_docs)\n","    return num_retrieved_relevant / num_retrieved if num_retrieved > 0 else 0\n","\n","\n","def calculate_relevant_count(retrieved_docs: list, query_docs: list) -> int:\n","    \"\"\"\n","    Find the number of intersected documents between `retrieved_docs` and `query_docs`\n","\n","    Args:\n","        retrieved_docs: Al list of doc_id that returned from matching\n","        query_docs: A list of doc_id belonging to a qid from qrel file\n","\n","    Returns:\n","        Number of shared results\n","    \"\"\"\n","    intersect_values = np.intersect1d(retrieved_docs, query_docs)\n","    matched_count = len(intersect_values)\n","    return matched_count\n","\n","\n","def average_precision(retrieved: list, relevant: list):\n","    p_sum = 0\n","    num_of_relevant = 0\n","    for i in range(10):\n","        k = i + 1\n","\n","        # get the doc_id's for the current\n","        relevant_docs = [doc['doc_id'] for doc in relevant]\n","\n","        p_at_k = precision_at_k(relevant_docs, retrieved, k)\n","        # print(f'P@{k} : {p_at_k}')\n","        # get the k document id\n","        k_doc_id = retrieved[:k][-1]\n","\n","        # get the rel(k)\n","        rel_at_k = get_rel_from_list(relevant, k_doc_id)\n","\n","        if rel_at_k > 0:\n","            num_of_relevant += rel_at_k\n","        p_sum += p_at_k * rel_at_k\n","\n","    return p_sum / num_of_relevant if num_of_relevant > 0 else 0\n","\n","\n","def get_rel_from_list(list_of_rel, doc_id) -> int:\n","    \"\"\"Get the rel for a given doc_id from a list of {'doc_id': 'NCT00445783', 'rel': 1}, ...\"\"\"\n","\n","    for rel in list_of_rel:\n","        if rel['doc_id'] == doc_id:\n","            return rel['rel']\n","\n","    return 0\n","\n","\n","def mean_average_precision(queries: dict, qrels: dict) -> float | int:\n","    ap_sum = 0\n","    for qid, query_results in queries.items():\n","        if qid == 10:\n","            continue\n","        # print(f'query number {qid} : ')\n","        val = average_precision(queries[qid], qrels[qid])\n","        # print(f'******* Average Precision : {val}')\n","        ap_sum += val\n","        # print('-------------------------------------')\n","    return ap_sum / len(queries)\n"],"metadata":{"id":"XiiY-7_danAP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"xTYKVD6YZKnH"}},{"cell_type":"code","source":["import csv\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","\n","\n","def get_queries_results(matrix)->dict:\n","    results = {}\n","    with (open(queries_path, 'r') as csvfile):\n","        reader = csv.reader(csvfile)\n","        for query_id, query in reader:\n","            if int(query_id) == 10:\n","                pass\n","            q = text_processor(query)\n","            qv = vectorizer.transform([q])\n","            similarity_scores = cosine_similarity(qv,matrix).flatten()\n","            sorted_indices = np.argsort(similarity_scores)[::-1]\n","            ranked_indices = sorted_indices[:10]\n","            ranked_indices = [num + 1 for num in ranked_indices]\n","            data = get_from_db(ranked_indices)\n","            docs_ids = [item['doc_id'] for item in data]\n","            results[int(query_id)] = docs_ids\n","    csvfile.close()\n","    return results\n"],"metadata":{"id":"PtNq5MaBYk8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vectors pickles"],"metadata":{"id":"haZNG5YScZy2"}},{"cell_type":"code","source":["def load_pickle_file(file_path):\n","    with open(file_path, \"rb\") as f:\n","        return pickle.load(f)\n","\n","titles_matrix = load_pickle_file('/content/drive/MyDrive/Dream Graduation Project/IR/titles_matrix.pickle')\n","summaries_matrix = load_pickle_file('/content/drive/MyDrive/Dream Graduation Project/IR/summaries_matrix.pickle')\n","conditions_matrix = load_pickle_file('/content/drive/MyDrive/Dream Graduation Project/IR/conditions_matrix.pickle')\n","descriptions_matrix = load_pickle_file('/content/drive/MyDrive/Dream Graduation Project/IR/descriptions_matrix.pickle')\n","eligibilities_matrix = load_pickle_file('/content/drive/MyDrive/Dream Graduation Project/IR/eligibilities_matrix.pickle')\n","keywords_matrix = load_pickle_file('/content/drive/MyDrive/Dream Graduation Project/IR/keywords_matrix.pickle')\n"],"metadata":{"id":"CfSYegNjcZFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"aQHoNyDV34YB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["initial_weights = {\n","    'title': 4,\n","    'description': 1,\n","    'summary': 3,\n","    'eligibility': 1,\n","    'keyword': 5,\n","    'condition':3\n","}\n","best_weights = initial_weights.copy()\n","best_map = -1\n","num_iterations = 100\n","learning_rate = 2\n","\n","\n","# Compute the weighted sum of matrices directly using sparse matrices\n","weighted_sum =  (initial_weights['title'] * titles_matrix +\n","                initial_weights['description'] * descriptions_matrix +\n","                initial_weights['summary'] * summaries_matrix +\n","                initial_weights['eligibility'] * eligibilities_matrix +\n","                initial_weights['keyword'] * keywords_matrix +\n","                initial_weights['condition'] * conditions_matrix)\n","\n","total_weight = sum(initial_weights.values())\n","main_matrix = weighted_sum / total_weight\n","\n","# Function to adjust weights slightly\n","def adjust_weights(weights, learning_rate):\n","    new_weights = weights.copy()\n","    for key in new_weights:\n","        adjustment = learning_rate * np.random.randn()\n","        new_weights[key] += adjustment\n","        # Ensure weights are non-negative\n","        new_weights[key] = max(new_weights[key], 0)\n","    return new_weights\n","\n","for iteration in range(num_iterations):\n","    new_weights = adjust_weights(initial_weights, learning_rate)\n","\n","    # Compute MAP with new weights\n","    weighted_sum =  (new_weights['title'] * titles_matrix +\n","                  new_weights['description'] * descriptions_matrix +\n","                  new_weights['summary'] * summaries_matrix +\n","                  new_weights['eligibility'] * eligibilities_matrix +\n","                  new_weights['keyword'] * keywords_matrix +\n","                  new_weights['condition'] * conditions_matrix)\n","    total_weight = sum(new_weights.values())\n","    matrix = weighted_sum / total_weight\n","\n","\n","    queries = get_queries_results(matrix)\n","\n","    current_map = mean_average_precision(queries,qrels)\n","\n","    # Check if current MAP is the best so far\n","    if current_map > best_map:\n","        best_map = current_map\n","        best_weights = new_weights.copy()\n","\n","    print(f\"Iteration {iteration + 1}/{num_iterations}, MAP: {current_map}, Weights: {new_weights}\")\n","\n","print(f\"Best MAP: {best_map}, Best Weights: {best_weights}\")\n","# write here\n","\n","\n","\n"],"metadata":{"id":"o-M-dNnnatJC"},"execution_count":null,"outputs":[]}]}